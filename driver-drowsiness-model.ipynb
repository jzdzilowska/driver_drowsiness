{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Drowsiness Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studies indicate that fatigue-related crashes account for about 20% of road accidents and even more on roads with no driving hour regulations. Driver detection systems, particularly those focusing on drowsiness detection, aim to address that alarming rate by monitoring drivers for signs of drowsiness and issuing timely alerts to prevent potential crashes.\n",
    "\n",
    "For our final project, we chose to develop a DDS by using the UTA Real-Life Drowsiness Dataset, which features diverse participants and comprehensive data. We will train a convolutional neural network (CNN) to analyze facial, eye, and mouth movements at different stages of drowsiness. The model will provide warnings and alerts based on the detected level of fatigue, with accuracy tests ensuring its reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements \n",
    "● TensorFlow: Developed by the Google Brain team for machine learning and artificial intelligence, Tensorflow has a allows for training and inference of deep neural networks.\n",
    "\n",
    "● Keras: Provides a Python interface for artificial neural networks (inbuilt python library).\n",
    "\n",
    "● Numpy: Used for scientific computing in Python. Provides support for arrays, matrices, and various mathematical functions to operate on them. \n",
    "\n",
    "● OpenCV: Machine learning and compiter vision library; contains >2500 algorhitms optimized for various CV tasks \n",
    "\n",
    "● Scikit-learn: Data mining, data analysis. In this project, used for splitting datasets. \n",
    "\n",
    "● Pandas: Data manipulation and analysis library. Used to create dataframes associating frames with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-26T08:46:01.934976Z",
     "iopub.status.busy": "2024-11-26T08:46:01.934499Z",
     "iopub.status.idle": "2024-11-26T08:46:03.940620Z",
     "shell.execute_reply": "2024-11-26T08:46:03.938637Z",
     "shell.execute_reply.started": "2024-11-26T08:46:01.934917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tg\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Adjustments \n",
    "Deletion of all previous files and checks for appropriate directories is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:51:25.229177Z",
     "iopub.status.busy": "2024-11-26T08:51:25.228757Z",
     "iopub.status.idle": "2024-11-26T08:51:26.424363Z",
     "shell.execute_reply": "2024-11-26T08:51:26.422432Z",
     "shell.execute_reply.started": "2024-11-26T08:51:25.229144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: /kaggle/working/*\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /kaggle/working/*\n",
    "folder1 = \"folder1\"\n",
    "folder2 = \"folder2\"\n",
    "\n",
    "if not os.path.exists(folder1):\n",
    "    os.mkdir(folder1)\n",
    "if not os.path.exists(folder2):\n",
    "    os.mkdir(folder2)\n",
    "\n",
    "# Change directory\n",
    "os.chdir('/kaggle/working/folder1')\n",
    "#/kaggle/input/uta-reallife-drowsiness-dataset/Fold1_part1/Fold1_part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing for Frame Extraction\n",
    "Code below processes video files from the UTA directory mentioned above to extract frames at specified intervals, and save them as images (for training, validation, and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:50:50.373080Z",
     "iopub.status.busy": "2024-11-26T08:50:50.372666Z",
     "iopub.status.idle": "2024-11-26T08:51:11.525455Z",
     "shell.execute_reply": "2024-11-26T08:51:11.523893Z",
     "shell.execute_reply.started": "2024-11-26T08:50:50.373049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input/uta-reallife-drowsiness-dataset/Fold1_part1'):\n",
    "    for filename in filenames:\n",
    "        participant = 1\n",
    "        pathway = os.path.join(dirname, filename)\n",
    "        print(os.path.join(dirname, filename))\n",
    "        cam = cv2.VideoCapture(pathway)\n",
    "\n",
    "        if not os.path.exists('/kaggle/working/frames'):\n",
    "            os.mkdir('/kaggle/working/frames')\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cam.read() # reading from frame\n",
    "            vid = 0\n",
    "            if ret:\n",
    "                print(\"next\")\n",
    "                if cam.get(0) % 3000 == 0: # capturing frame every 3000 ms\n",
    "                    print(\"captured\")\n",
    "                    currentframe = cam.get(5)\n",
    "                    # if video is still left, continue creating images \n",
    "                    name = 'Participant_' + str(participant) + \"Vid_\" + str(vid) + '_Frame_' + str(currentframe) + '.jpg' # vid = 0, 5, 10; alert, neutral, drowsy\n",
    "                    cv2.imwrite(name, frame) #writing the extracted images \n",
    "            else:\n",
    "                break\n",
    "            vid += 5\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        participant += 1\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:52:34.172717Z",
     "iopub.status.busy": "2024-11-19T09:52:34.172261Z",
     "iopub.status.idle": "2024-11-19T09:52:34.182489Z",
     "shell.execute_reply": "2024-11-19T09:52:34.181063Z",
     "shell.execute_reply.started": "2024-11-19T09:52:34.172673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(f\"ERROR: creating directory with name {path}\")\n",
    "\n",
    "def save_frame(video_path, save_dir, gap=10):\n",
    "    name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    save_path = os.path.join(save_dir, name)\n",
    "    create_dir(save_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            cap.release()\n",
    "            break\n",
    "        if idx == 0:\n",
    "            cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "        else:\n",
    "            if idx % gap == 0:\n",
    "                cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "        idx += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_paths = glob(\"videos/*\")\n",
    "    save_dir = \"save\"\n",
    "\n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame - Class (Label) Association\n",
    "Frames captured are associated with \"not drowsy\", \"neutral\", and \"drowsy\" classes, based on the 'vid' label within the parsed filename. They're later saved to a pandas dataframe for training, validating, and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    for i, part in enumerate(parts):\n",
    "        if part.lower() == 'vid':\n",
    "            label = int(parts[i + 1])\n",
    "            if label == 0:\n",
    "                return 'not_drowsy'\n",
    "            elif label == 5:\n",
    "                return 'neutral'\n",
    "            elif label == 10:\n",
    "                return 'drowsy'\n",
    "            else:\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             filepath       label\n",
      "0   /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "1   /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "2   /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "3   /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "4   /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "5   /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "6   /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "7   /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "8   /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "9   /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "10  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "11  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "12  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "13  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "14  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "15  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "16  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "17  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "18  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "19  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "20  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "21  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "22  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "23  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "24  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "25  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "26  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "27  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "28  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "29  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "30  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "31  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "32  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "33  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "34  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "35  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "36  /Users/zdzilowska/Desktop/ANN final/frames/Par...  not_drowsy\n",
      "37  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "38  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "39  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "40  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "41  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "42  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "43  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "44  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "45  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "46  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "47  /Users/zdzilowska/Desktop/ANN final/frames/Par...     neutral\n",
      "48  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n",
      "49  /Users/zdzilowska/Desktop/ANN final/frames/Par...      drowsy\n"
     ]
    }
   ],
   "source": [
    "# ! Universalize the working directory\n",
    "working_directory = '/Users/zdzilowska/Desktop/ANN final/frames'\n",
    "\n",
    "def create_dataframe(image_dir):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                label = parse_filename(file)\n",
    "                if label:\n",
    "                    data.append((os.path.join(root, file), label))\n",
    "    return pd.DataFrame(data, columns=['filepath', 'label'])\n",
    "\n",
    "df = create_dataframe(working_directory)\n",
    "print(df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Augmentation\n",
    "The dataset is split into training, validation, and testing sets. The frames are then rescaled, as well as augmented for the training dataset to increase the variety of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 validated image filenames belonging to 3 classes.\n",
      "Found 181 validated image filenames belonging to 3 classes.\n",
      "Found 182 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialization of the train, validation, and test datasets extracted from the UTA RealLife Drowsiness Dataset. \n",
    "train_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, stratify=train_val_df['label'], random_state=42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=0.2)\n",
    "\n",
    "# Artificially increases size of the training dataset; ensures a wider range of imgs. \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition, Compilation, and Training\n",
    "The model architecture is defined using a pre-trained (on ImageNet) VGG16 base model. The top layers are excluded and the input shape is specified to match the dimensions of our input data. Custom layers are then added for the 3-class classification. To prevent the weights of the pre-trained VGG16 base model from being updated during training, we freeze all the layers of the base model, after which the model is compiled, and trained using the training and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdzilowska/miniconda3/envs/finalann/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.6970 - loss: 5.7686 - val_accuracy: 0.9625 - val_loss: 0.3552\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/16\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 3s/step - accuracy: 0.9062 - loss: 1.0289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdzilowska/miniconda3/envs/finalann/lib/python3.11/site-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 896ms/step - accuracy: 0.9062 - loss: 1.0289 - val_accuracy: 0.9688 - val_loss: 0.2779\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 4s/step - accuracy: 0.9570 - loss: 0.2605 - val_accuracy: 0.9937 - val_loss: 0.0132\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 946ms/step - accuracy: 1.0000 - loss: 4.8315e-06 - val_accuracy: 0.9875 - val_loss: 0.0259\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - accuracy: 0.9858 - loss: 0.0390 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 882ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3s/step - accuracy: 0.9964 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0073\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 906ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 7.5266e-04 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 896ms/step - accuracy: 1.0000 - loss: 0.0178 - val_accuracy: 1.0000 - val_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2981b3390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # 3 classes: 0 - not_drowsy, 5 - drowsy, 10 - neutral\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# The base is freezed\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ! Actual training\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('drowsiness_detection_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "Obtained results: 0.0897449404001236 test loss, 0.9937499761581421 test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3s/step - accuracy: 0.9966 - loss: 0.0506\n",
      "Test loss: 0.0897449404001236\n",
      "Test accuracy: 0.9937499761581421\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2839089,
     "sourceId": 4895821,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "finalann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
