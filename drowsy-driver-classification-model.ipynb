{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Drowsiness Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studies indicate that fatigue-related crashes account for about 20% of road accidents and even more on roads with no driving hour regulations. Driver detection systems, particularly those focusing on drowsiness detection, aim to address that alarming rate by monitoring drivers for signs of drowsiness and issuing timely alerts to prevent potential crashes.\n",
    "\n",
    "For our final project, we chose to develop a DDS that will utilize two two convolutional neural networks (CNN). One will be trained on the UTA Dataset (comprised of drowsy/neutral/alert driver images), and the other on Behavioral Signs Dataset (containing images of closed/open eyes, and yawning/not yawning drivers); both model's predictions will be layer combined to output the final result. The combined model will then provide warnings and alerts based on the detected level of fatigue in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements \n",
    "- TensorFlow: Developed by the Google Brain team for machine learning and artificial intelligence, Tensorflow has a allows for training and inference of deep neural networks.\n",
    "\n",
    "- Keras: Provides a Python interface for artificial neural networks (inbuilt python library).\n",
    "\n",
    "- Numpy: Used for scientific computing in Python. Provides support for arrays, matrices, and various mathematical functions to operate on them. \n",
    "\n",
    "- OpenCV: Machine learning and compiter vision library; contains >2500 algorhitms optimized for various CV tasks; allows for webcam access.\n",
    "\n",
    "- Scikit-learn: Data mining, data analysis. In this project, used for splitting datasets. \n",
    "\n",
    "- Pandas: Data manipulation and analysis library. Used to create dataframes associating frames with their labels.\n",
    "\n",
    "- Kagglehub: For downloading Kaggle datasets \n",
    "\n",
    "- Visualkeras: For network visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T11:41:49.280953Z",
     "iopub.status.busy": "2024-12-04T11:41:49.280282Z",
     "iopub.status.idle": "2024-12-04T11:42:05.787206Z",
     "shell.execute_reply": "2024-12-04T11:42:05.786103Z",
     "shell.execute_reply.started": "2024-12-04T11:41:49.280910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import cv2\n",
    "import os\n",
    "import kagglehub\n",
    "import visualkeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRESETS \n",
    "# Current working directory \n",
    "working_dir = os.getcwd()\n",
    "# UTA DD Dataset directory \n",
    "uta_dir = working_dir + '/frames'\n",
    "# Behavioral Signs Dataset directory\n",
    "behavior_dir = working_dir + '/behavioral_signs_frames'\n",
    "# OpenCV cascade for face detection\n",
    "face_cascade_path = working_dir + '/haarcascade_frontalface_default.xml'\n",
    "# Image size for VGG16\n",
    "IMG_SIZE = 224 \n",
    "# Initializing face cascade (will be used to detect faces in the images)\n",
    "face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "# Initializing the VGG16 model (will be used for feature extraction, not the final prediction)\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame - Class (Label) Association\n",
    "### Appending Labels for the UTA Dataset \n",
    "Frames captured are associated with **0 = not drowsy or neutral, 1 = drowsy classes**, based on the 'vid' label within the parsed filename. They're then processed (faces of drivers are extracted and saved as pixel values) and added to a pandas dataframe for training, validating, and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:42:05.789863Z",
     "iopub.status.busy": "2024-12-04T11:42:05.789259Z",
     "iopub.status.idle": "2024-12-04T11:42:05.796754Z",
     "shell.execute_reply": "2024-12-04T11:42:05.795501Z",
     "shell.execute_reply.started": "2024-12-04T11:42:05.789826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    for i, part in enumerate(parts):\n",
    "        if part.lower() == 'vid':\n",
    "            label = int(parts[i + 1])\n",
    "            if label == 0 or label == 5:\n",
    "                return 0\n",
    "            elif label == 10: \n",
    "                return 1\n",
    "            else:\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:42:05.798772Z",
     "iopub.status.busy": "2024-12-04T11:42:05.798445Z",
     "iopub.status.idle": "2024-12-04T11:42:09.415393Z",
     "shell.execute_reply": "2024-12-04T11:42:09.414185Z",
     "shell.execute_reply.started": "2024-12-04T11:42:05.798744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[[[117, 136, 151], [117, 136, 151], [117, 136,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[30, 37, 46], [31, 38, 46], [31, 37, 43], [3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>[[[53, 46, 43], [54, 46, 45], [54, 46, 46], [5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[[82, 108, 122], [83, 109, 123], [83, 108, 12...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[[[110, 124, 136], [118, 132, 144], [118, 132,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              features  label\n",
       "239  [[[117, 136, 151], [117, 136, 151], [117, 136,...      0\n",
       "2    [[[30, 37, 46], [31, 38, 46], [31, 37, 43], [3...      0\n",
       "820  [[[53, 46, 43], [54, 46, 45], [54, 46, 46], [5...      0\n",
       "29   [[[82, 108, 122], [83, 109, 123], [83, 108, 12...      1\n",
       "53   [[[110, 124, 136], [118, 132, 144], [118, 132,...      1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to process the images, detect faces, crop, and use VGG16 for feature extraction.\n",
    "# Returns the extracted features and labels. \n",
    "def extract_features_uta(image_dir, face_cascade, vgg16_model):\n",
    "    data = []\n",
    "    labels = []  \n",
    "    # Iterating through the image directories\n",
    "    for root, dirs, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):  \n",
    "                img_path = os.path.join(root, file)\n",
    "                \n",
    "                img = cv2.imread(img_path)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Detecting faces\n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                \n",
    "                # Processing each detected face\n",
    "                for (x, y, w, h) in faces:\n",
    "                    face = img[y:y+h, x:x+w]  # Cropping the face\n",
    "                    # Resizing the face to the input size expected by VGG16\n",
    "                    resized_face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "                    label = parse_filename(file)  \n",
    "                    data.append([resized_face, label])\n",
    "    return pd.DataFrame(data, columns=['features', 'label'])\n",
    "    \n",
    "df_drowsiness = extract_features_uta(uta_dir, face_cascade, vgg16_model)\n",
    "\n",
    "# Sanity check\n",
    "df_drowsiness.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending Labels for the Behavioral Signs Dataset\n",
    "To adjust the complexity of our final (combined) model, we will train a second CNN that utilizes the Behavioral Signs Dataset. The behavioral_signs_frames folder contains labeled images of open and closed eyes, as well as yawning and non-yawning drivers. This added data will significantly enhance the model's ability to generalize, addressing the overfitting issue encountered when using only the UTA dataset frames in our initial model (will be used as features in the final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>behavior_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>[[[26, 32, 55], [26, 32, 55], [27, 33, 56], [2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>[[[57, 63, 52], [57, 63, 52], [57, 63, 52], [5...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>[[[157, 151, 138], [158, 152, 139], [159, 156,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>[[[76, 91, 94], [40, 53, 57], [71, 84, 87], [1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>[[[5, 13, 26], [7, 15, 28], [10, 18, 32], [17,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               features  behavior_label\n",
       "409   [[[26, 32, 55], [26, 32, 55], [27, 33, 56], [2...               0\n",
       "1168  [[[57, 63, 52], [57, 63, 52], [57, 63, 52], [5...               3\n",
       "1320  [[[157, 151, 138], [158, 152, 139], [159, 156,...               1\n",
       "820   [[[76, 91, 94], [40, 53, 57], [71, 84, 87], [1...               2\n",
       "1681  [[[5, 13, 26], [7, 15, 28], [10, 18, 32], [17,...               1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'closed': 0, 'open': 1, 'no_yawn': 2, 'yawn': 3}\n",
    "\n",
    "# Function to preprocess images and detect faces from the behavioral signs dataset. Returns a DataFrame with the processed data.\n",
    "def extract_features_behavior(image_dir, face_cascade_path, label_map):\n",
    "    data = []\n",
    "    # No need to extract faces in the case of 'open' and 'closed' labels\n",
    "    for label in os.listdir(image_dir):\n",
    "        # Constructing the path to the label folder \n",
    "        label_folder_path = os.path.join(image_dir, label)\n",
    "        if label in ['open', 'closed']:  \n",
    "            if os.path.isdir(label_folder_path):\n",
    "                for image_name in os.listdir(label_folder_path):\n",
    "                    img_array = cv2.imread(os.path.join(label_folder_path, image_name), cv2.IMREAD_COLOR)\n",
    "                    array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    data.append([array, label_map[label]])\n",
    "        else: # if label is 'yawn' or 'no_yawn'\n",
    "            if os.path.isdir(label_folder_path):\n",
    "                for image_name in os.listdir(label_folder_path):\n",
    "                    image_path = os.path.join(label_folder_path, image_name)\n",
    "\n",
    "                    # Load the image, skip if invalid\n",
    "                    img = cv2.imread(image_path)\n",
    "                    if img is None:\n",
    "                        continue \n",
    "\n",
    "                    # Convert to grayscale (required for face detection)\n",
    "                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "                    # If a face is detected, crop and resize the face\n",
    "                    for (x, y, w, h) in faces:\n",
    "                        face = img[y:y+h, x:x+w]  # Cropping the face\n",
    "                        resized_face = cv2.resize(face, (IMG_SIZE, IMG_SIZE)) \n",
    "                        behavior_label = label_map[label]\n",
    "                        data.append([resized_face, behavior_label])\n",
    "    return pd.DataFrame(data, columns=['features', 'behavior_label'])\n",
    "\n",
    "# Create the DataFrame with face detection and preprocessing\n",
    "df_behavior = extract_features_behavior(behavior_dir, face_cascade_path, label_map) # Has shape (224, 224, 3)\n",
    "\n",
    "# Sanity check \n",
    "df_behavior.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Augmentation \n",
    "### UTA Model\n",
    "The UTA RealLife Drowsiness Dataset is split into training, validation, and testing sets, and later converted to tensorflow datasets. The frames have been previously rescaled and augmented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:42:09.440520Z",
     "iopub.status.busy": "2024-12-04T11:42:09.440239Z",
     "iopub.status.idle": "2024-12-04T11:42:10.492056Z",
     "shell.execute_reply": "2024-12-04T11:42:10.491250Z",
     "shell.execute_reply.started": "2024-12-04T11:42:09.440491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Splittig based on the label. 30% test, 25% validation, 45% training\n",
    "train_val_df, test_df = train_test_split(df_drowsiness, test_size=0.3, stratify=df_drowsiness['label'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, stratify=train_val_df['label'], random_state=42)\n",
    "\n",
    "# Converting DataFrame columns to TensorFlow datasets\n",
    "def df_to_dataset(df, batch_size=32):\n",
    "  features = np.stack(df['features'].values)\n",
    "  labels = df['label'].values\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "  dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset\n",
    "\n",
    "# Creating TensorFlow datasets\n",
    "train_dataset = df_to_dataset(train_df)\n",
    "val_dataset = df_to_dataset(val_df)\n",
    "test_dataset = df_to_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Model\n",
    "The behavioral model is split into datasets as the UTA model, with the features converted into numPy arrays to match the expected network input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features and labels\n",
    "X = np.stack(df_behavior['features'].values)  # Converting features to a numpy array. Will have shape (tot_imgs, 224, 224, 3)\n",
    "y = np.stack(df_behavior['behavior_label'].values) # Converting labels to a numpy array. Will have shape (tot_imgs,)\n",
    "\n",
    "# Splitting into train, validation, and test sets.\n",
    "X_train_beh, X_test_beh, y_train_beh, y_test_beh = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "X_val_beh, X_test_beh, y_val_beh, y_test_beh = train_test_split(X_test_beh, y_test_beh, test_size=0.5, random_state=42)\n",
    "\n",
    "# Data augmentation for improved generalization\n",
    "train_behavioral = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "validate_behavioral = ImageDataGenerator(rescale=1/255)\n",
    "test_behavioral = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Setting the batch size for improved generalization\n",
    "train_behavioral = train_behavioral.flow(np.array(X_train_beh), y_train_beh, shuffle=False)\n",
    "validate_behavioral = validate_behavioral.flow(np.array(X_val_beh), y_val_beh, shuffle=False)\n",
    "test_behavioral = test_behavioral.flow(np.array(X_test_beh), y_test_beh, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition, Compilation, and Training\n",
    "### UTA Model D, C & T \n",
    "The model architecture is defined using a pre-trained (on ImageNet) VGG16 base model. The top layers are excluded and the input shape is specified to match the dimensions of our input data. Custom layers are then added for the 3-class classification. To prevent the weights of the pre-trained VGG16 base model from being updated during training, we freeze all the layers of the base model, after which the model is compiled, and trained using the training and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T11:42:10.493764Z",
     "iopub.status.busy": "2024-12-04T11:42:10.493375Z",
     "iopub.status.idle": "2024-12-04T12:47:52.036098Z",
     "shell.execute_reply": "2024-12-04T12:47:52.034888Z",
     "shell.execute_reply.started": "2024-12-04T11:42:10.493719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 12s/step - accuracy: 0.8225 - loss: 3.7657 - val_accuracy: 0.9329 - val_loss: 4.5201\n",
      "Epoch 2/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 11s/step - accuracy: 0.9866 - loss: 0.5575 - val_accuracy: 0.9799 - val_loss: 1.8185\n",
      "Epoch 3/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 10s/step - accuracy: 0.9971 - loss: 0.0595 - val_accuracy: 0.9664 - val_loss: 1.7039\n",
      "Epoch 4/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 10s/step - accuracy: 0.9995 - loss: 0.0196 - val_accuracy: 0.9799 - val_loss: 1.3128\n",
      "Epoch 5/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 10s/step - accuracy: 1.0000 - loss: 6.8437e-06 - val_accuracy: 0.9866 - val_loss: 1.3736\n",
      "Epoch 6/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 11s/step - accuracy: 0.9992 - loss: 0.0678 - val_accuracy: 0.9866 - val_loss: 1.3257\n",
      "Epoch 7/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 1.1004e-21 - val_accuracy: 0.9799 - val_loss: 1.3399\n",
      "Epoch 8/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 2.5186e-18 - val_accuracy: 0.9799 - val_loss: 1.4659\n",
      "Epoch 9/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 2.1291e-11 - val_accuracy: 0.9799 - val_loss: 1.4977\n",
      "Epoch 10/10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 11s/step - accuracy: 0.9950 - loss: 0.2225 - val_accuracy: 0.9799 - val_loss: 1.5308\n"
     ]
    }
   ],
   "source": [
    "# Loading the VGG16 model (pretrained on ImageNet)\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Adding the fully connected layers for classification\n",
    "model = Sequential([\n",
    "    vgg16_base,\n",
    "    Flatten(), \n",
    "    Dense(256, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.5),  # Regularization\n",
    "    Dense(1, activation='sigmoid') # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Freezing  the base model layers to keep the pretrained weights\n",
    "for layer in vgg16_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T12:47:52.037859Z",
     "iopub.status.busy": "2024-12-04T12:47:52.037541Z",
     "iopub.status.idle": "2024-12-04T12:47:52.210514Z",
     "shell.execute_reply": "2024-12-04T12:47:52.209618Z",
     "shell.execute_reply.started": "2024-12-04T12:47:52.037830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('drowsiness_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Model D, C & T\n",
    "The model is designed to classify images into yawning, not yawning, open eyes, and closed eyes categories. The model uses **convolutional layers** which help to learn spatial features in the images (higher num of filters in initial layers captures low-level edges and textures, while the decreasing spatial dimension deeper in the network captures more abstract features). 512 and 128 have been found to work well for image classification - VGG16 has similar configuration. **MaxPooling** reduces the spatial dimensions of the images (-> model focuses on the most important features), **dropout** helps prevent overfitting, and the final **softmax** layer outputs the probabilities for each of the four classes. Sparse categorical crossentropy automatically handles integers ranging from [0,3] to convert them to a one-hot encoded format. \n",
    "The class with the highest probability is the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zdzilowska/miniconda3/envs/dis/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/zdzilowska/miniconda3/envs/dis/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 6s/step - accuracy: 0.3925 - loss: 1.4297 - val_accuracy: 0.7370 - val_loss: 0.6911\n",
      "Epoch 2/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 7s/step - accuracy: 0.5972 - loss: 0.9039 - val_accuracy: 0.7509 - val_loss: 0.5815\n",
      "Epoch 3/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 7s/step - accuracy: 0.6492 - loss: 0.8235 - val_accuracy: 0.8097 - val_loss: 0.4506\n",
      "Epoch 4/20\n",
      "\u001b[1m37/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m39s\u001b[0m 7s/step - accuracy: 0.6727 - loss: 0.7496"
     ]
    }
   ],
   "source": [
    "model_beh = Sequential([\n",
    "    Conv2D(256, (3, 3), activation='relu', input_shape=X_train_beh.shape[1:]),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model_beh.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Since s_c_c automatically one-hot encodes the 4 labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_beh = model_beh.fit(\n",
    "    train_behavioral,\n",
    "    validation_data=validate_behavioral,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beh.save_weights('behavioral_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture \n",
    "### UTA Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import visualkeras\n",
    "\n",
    "plot_model(model, to_file='model_drowsiness.png', show_shapes=True, show_layer_names=True)\n",
    "visualkeras.layered_view(model, legend=True, draw_volume=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_beh, to_file='model_behavioral.png', show_shapes=True, show_layer_names=True)\n",
    "visualkeras.layered_view(model_beh, legend=True, draw_volume=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "### UTA Model Performance\n",
    "With the accuracy surprisingly high, we've checked the appropriateness of labels (correct - the dataset is balanced; same amount of drowsy, and not drowsy images), and for potential data leakage issues (none - we've verified the integrity of train-test splits, all of the dataframe's feature column cells are unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T12:47:52.211972Z",
     "iopub.status.busy": "2024-12-04T12:47:52.211651Z",
     "iopub.status.idle": "2024-12-04T12:53:15.257534Z",
     "shell.execute_reply": "2024-12-04T12:53:15.256305Z",
     "shell.execute_reply.started": "2024-12-04T12:47:52.211942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9s/step - accuracy: 1.0000 - loss: 6.5497e-13\n",
      "Test loss: 4.011881701414949e-13\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=tf.data.experimental.cardinality(test_dataset).numpy())\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Accuracy and loss plots\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "# Highest testing accuracy and its index\n",
    "max_val_acc = max(val_accuracy)\n",
    "max_val_acc_index = val_accuracy.index(max_val_acc)\n",
    "min_val_acc = min(val_accuracy)\n",
    "max_accuracy = max(accuracy)\n",
    "min_loss =  min(loss)\n",
    "min_val_loss = min(val_loss)\n",
    "plt.plot(max_val_acc_index, max_val_acc, marker='o', color='lightpink')\n",
    "\n",
    "# Training and testing accuracy over epochs\n",
    "plt.plot(epochs, accuracy, \"g\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"r\", label=\"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Testing Accuracy of the UTA Model\\nHighest Testing Accuracy: {:.2f}%\".format(max_val_acc*100))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting loss over epochs\n",
    "plt.plot(epochs, loss, \"g\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r\", label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Testing Loss of the UTA Model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral CNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history_beh.history['accuracy']\n",
    "val_accuracy = history_beh.history['val_accuracy']\n",
    "loss = history_beh.history['loss']\n",
    "val_loss = history_beh.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "max_val_acc = max(val_accuracy)\n",
    "max_val_acc_index = val_accuracy.index(max_val_acc)\n",
    "min_val_acc = min(val_accuracy)\n",
    "max_accuracy = max(accuracy)\n",
    "min_loss =  min(loss)\n",
    "min_val_loss = min(val_loss)\n",
    "plt.plot(max_val_acc_index, max_val_acc, marker='o', color='lightpurple')\n",
    "\n",
    "plt.plot(epochs, accuracy, \"g\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"r\", label=\"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Testing Accuracy of the Behavioral Model\\nHighest Testing Accuracy: {:.2f}%\".format(max_val_acc*100))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"g\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r\", label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Testing Loss of the Behavioral Model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Driver Drowsiness Detection \n",
    "Using the models defined above (which was ), we'll now implement a real-time DDS based on a webcam. The code captures live video frames using OpenCV, processes them to match the input requirements of the trained model, and performs predictions to determine drowsy vs alert state. The model's predictions are overlaid onto the video feed in real time, displaying the driver's status and confidence scores. \n",
    "\n",
    "*The system is interactive, allowing the user to view the annotated video feed and terminate the program by pressing the 'q' key.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding both models to omit steps above, with the weights already generated\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  \n",
    "model_drowsiness = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model_behavior = Sequential([\n",
    "  Conv2D(128, (3, 3), activation='relu', input_shape=(X_train_beh.shape[1:])),  \n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(64, (3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dropout(0.5),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(4, activation='softmax')  \n",
    "])\n",
    "model_behavior.compile(\n",
    "  optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_drowsiness_path = \"drowsiness_weights.weights.h5\"\n",
    "model_drowsiness.load_weights(model_drowsiness_path)\n",
    "\n",
    "model_behavior_path = \"behavioral_weights.weights.h5\"\n",
    "model_behavior.load_weights(model_behavior_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit the webcam stream.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "7\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "9\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "13\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "16\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "17\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step\n",
      "18\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step\n",
      "19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
      "20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
      "21\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step\n",
      "22\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n",
      "23\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
      "24\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
      "26\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n",
      "27\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
      "28\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
      "29\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step\n",
      "31\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step\n",
      "32\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step\n",
      "33\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step\n",
      "34\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "35\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step\n",
      "36\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
      "37\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "38\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step\n",
      "39\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step\n",
      "40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
      "41\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\n",
      "42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "43\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step\n",
      "45\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step\n",
      "46\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
      "47\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "48\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\n",
      "49\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step\n",
      "50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step\n",
      "51\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\n",
      "52\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
      "53\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step\n",
      "54\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step\n",
      "55\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
      "56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step\n",
      "57\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n"
     ]
    }
   ],
   "source": [
    "# Configuration for alert system\n",
    "consecutive_drowsy_frames = 30  # Num of consecutive \"drowsy\" frames to trigger alert\n",
    "drowsy_counter = 0\n",
    "\n",
    "# Defining the preprocessing function to match the model's input requirements. Returns a single adjusted frame. \n",
    "def preprocess_frame(frame, face_cascade=face_cascade):\n",
    "  # Read in the image\n",
    "  img = cv2.imread(frame, cv2.IMREAD_COLOR)\n",
    "  # Normalize \n",
    "  img = img / 255\n",
    "  # Resize to match the size used during training\n",
    "  resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "  # Reshape the image to match the input shape \n",
    "  return resized.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Starting the camera; 0 - default camera\n",
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "print(\"Press 'q' to quit the webcam stream.\")\n",
    "while cap.isOpened():\n",
    "  ret, frame = cap.read()  # Reading frame-by-frame\n",
    "  if not ret:\n",
    "    print(\"Failed to capture frame. Exiting...\")\n",
    "    break\n",
    "\n",
    "  processed_frame = preprocess_frame(frame)\n",
    "\n",
    "  # Making a prediction\n",
    "  prediction_drowsy = model_drowsiness.predict(processed_frame)\n",
    "  drowsy_label = np.argmax(prediction_drowsy) # Index of the max value in the array of probabilities - max probability class\n",
    "  prediction_beh = model_behavior.predict(processed_frame)\n",
    "  beh_label = np.argmax(prediction_beh)\n",
    "\n",
    "  # Combine predictions. Final status will be drowsy if 30 cons frames are drowsy (/closed/yawn)\n",
    "  if drowsy_label == 0 or (beh_label in [1, 3]):  # drowsy or closed/yawn\n",
    "    drowsy_counter += 1\n",
    "  else: \n",
    "    drowsy_counter = 0\n",
    "\n",
    "  print(drowsy_counter)\n",
    "\n",
    "  # Checking if alert conditions are met\n",
    "  if drowsy_counter >= consecutive_drowsy_frames:\n",
    "    status = \"Drowsiness Detected\"\n",
    "    color = (0, 0, 255)  \n",
    "  else :\n",
    "    status = \"No Drowsiness Detected\"\n",
    "    color = (0, 255, 0)\n",
    "\n",
    "  # Annotating the frame\n",
    "  cv2.putText(frame, f\"Status: {status}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "  cv2.putText(frame, f\"Confidence: {drowsy_prob:.2f}\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "  # Displaying the frame with annotations\n",
    "  cv2.imshow(\"Drowsy Driver Detection\", frame)\n",
    "\n",
    "  # Breaking the loop once 'q' is pressed\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Total frames processed: {frame_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix ###\n",
    "Finish later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Alert\", \"Drowsy\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6213409,
     "sourceId": 10079118,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
