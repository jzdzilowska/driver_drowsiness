{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10079118,"sourceType":"datasetVersion","datasetId":6213409},{"sourceId":10156669,"sourceType":"datasetVersion","datasetId":6270986}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Driver Drowsiness Detection System","metadata":{}},{"cell_type":"markdown","source":"Studies indicate that fatigue-related crashes account for about 20% of road accidents and even more on roads with no driving hour regulations. Driver detection systems, particularly those focusing on drowsiness detection, aim to address that alarming rate by monitoring drivers for signs of drowsiness and issuing timely alerts to prevent potential crashes.\n\nFor our final project, we chose to develop a DDS that will utilize a convolutional neural network (CNN) trained on the [UTA Dataset](https://www.kaggle.com/datasets/rishab260/uta-reallife-drowsiness-dataset) (comprised of drowsy/neutral/alert driver images). The model's prediction will then provide alerts based on the detected level of fatigue in real time through a webcam notification. \n\n*Dataset needs to be downloaded and stored in the root directory - frames from the UTA dataset have been extracted by us separately, through [code available on Kaggle](https://www.kaggle.com/code/jmash12/film-to-frames)*","metadata":{}},{"cell_type":"markdown","source":"### Requirements \n- TensorFlow: Developed by the Google Brain team for machine learning and artificial intelligence, Tensorflow has a allows for training and inference of deep neural networks.\n\n- Keras: Provides a Python interface for artificial neural networks (inbuilt python library).\n\n- Numpy: Used for scientific computing in Python. Provides support for arrays, matrices, and various mathematical functions to operate on them. \n\n- OpenCV: Machine learning and compiter vision library; contains >2500 algorhitms optimized for various CV tasks; allows for webcam access.\n\n- Scikit-learn: Data mining, data analysis. In this project, used for splitting datasets. \n\n- Pandas: Data manipulation and analysis library. Used to create dataframes associating frames with their labels.\n\n- Kagglehub: For downloading Kaggle datasets \n\n- Visualkeras: For network visualization\n","metadata":{}},{"cell_type":"code","source":"pip install visualkeras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:41:26.679902Z","iopub.execute_input":"2024-12-10T08:41:26.680332Z","iopub.status.idle":"2024-12-10T08:41:38.736098Z","shell.execute_reply.started":"2024-12-10T08:41:26.680296Z","shell.execute_reply":"2024-12-10T08:41:38.734683Z"}},"outputs":[{"name":"stdout","text":"Collecting visualkeras\n  Downloading visualkeras-0.1.4-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (10.3.0)\nRequirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (1.26.4)\nCollecting aggdraw>=1.3.11 (from visualkeras)\n  Downloading aggdraw-1.3.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (655 bytes)\nDownloading visualkeras-0.1.4-py3-none-any.whl (17 kB)\nDownloading aggdraw-1.3.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: aggdraw, visualkeras\nSuccessfully installed aggdraw-1.3.19 visualkeras-0.1.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport kagglehub\nimport visualkeras\nfrom tensorflow.keras import regularizers\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:41:41.729855Z","iopub.execute_input":"2024-12-10T08:41:41.730445Z","iopub.status.idle":"2024-12-10T08:41:41.771294Z","shell.execute_reply.started":"2024-12-10T08:41:41.730406Z","shell.execute_reply":"2024-12-10T08:41:41.769731Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# PRESETS \n# Current working directory \nworking_dir = '/kaggle/input/drowsy-driver-imagesonly/Drowsey Driver Images'\n# UTA DD Dataset directory (frames = participant_02_vid...)\nuta_dir = working_dir + '/frames' \n# Image size for VGG16\nIMG_SIZE = 224 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:59:46.200432Z","iopub.execute_input":"2024-12-09T16:59:46.200886Z","iopub.status.idle":"2024-12-09T16:59:46.206418Z","shell.execute_reply.started":"2024-12-09T16:59:46.200851Z","shell.execute_reply":"2024-12-09T16:59:46.205224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Appending Labels for the UTA Dataset \nFrames captured are associated with **not drowsy, neutral, and drowsy classes**, based on the 'vid' label within the parsed filename. They're later saved to a pandas dataframe for training, validating, and testing.","metadata":{}},{"cell_type":"code","source":"#Get images and label them based on proper classification \ndef parse_filename(filename):\n    parts = filename.split('_')\n    for i, part in enumerate(parts):\n        if part.lower() == 'vid':\n            label = int(parts[i + 1])\n            if label == 0:\n                return 'not_drowsy'\n            elif label == 5:\n                return 'neutral'\n            elif label == 10:\n                return 'drowsy'\n            else:\n                return None\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-12-09T16:59:48.495650Z","iopub.execute_input":"2024-12-09T16:59:48.496074Z","iopub.status.idle":"2024-12-09T16:59:48.502414Z","shell.execute_reply.started":"2024-12-09T16:59:48.496041Z","shell.execute_reply":"2024-12-09T16:59:48.501358Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to process the images. \ndef create_dataframe(image_dir):\n    data = []\n    for root, dirs, files in os.walk(image_dir):\n        for file in files:\n            if file.endswith('.jpg'):\n                label = parse_filename(file)\n                if label:\n                    data.append((os.path.join(root, file), label))\n    return pd.DataFrame(data, columns=['filepath', 'label'])\n\ndf_drowsiness = create_dataframe(working_dir)\n\n# Sanity check\ndf_drowsiness.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T16:59:50.971779Z","iopub.execute_input":"2024-12-09T16:59:50.972915Z","iopub.status.idle":"2024-12-09T16:59:54.848150Z","shell.execute_reply.started":"2024-12-09T16:59:50.972860Z","shell.execute_reply":"2024-12-09T16:59:54.847038Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing and Augmenting the Model\nThe dataset is split into training, validation, and testing sets. The frames are then rescaled, as well as augmented for the training dataset to increase the variety of data.","metadata":{}},{"cell_type":"code","source":"# Initialization of the train, validation, and test datasets extracted from the UTA RealLife Drowsiness Dataset. \ntrain_val_df, test_df = train_test_split(df_drowsiness, test_size=0.2, stratify=df_drowsiness['label'], random_state=42)\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.25, stratify=train_val_df['label'], random_state=42)\n\ntrain_datagen = ImageDataGenerator(rescale=0.2)\nval_datagen = ImageDataGenerator(rescale=0.2)\ntest_datagen = ImageDataGenerator(rescale=0.2)\n\n# Artificially increases size of the training dataset; ensures a wider range of imgs. \ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='filepath',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical'\n)\n\nval_generator = val_datagen.flow_from_dataframe(\n    val_df,\n    x_col='filepath',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    test_df,\n    x_col='filepath',\n    y_col='label',\n    target_size=(224, 224),\n    batch_size=64,\n    class_mode='categorical',\n    shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T17:00:14.528326Z","iopub.execute_input":"2024-12-09T17:00:14.529309Z","iopub.status.idle":"2024-12-09T17:00:18.350095Z","shell.execute_reply.started":"2024-12-09T17:00:14.529264Z","shell.execute_reply":"2024-12-09T17:00:18.348971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Definition, Compilation, and Training\nThe model architecture is defined using a pre-trained (on ImageNet) VGG16 base model. The top layers are excluded and the input shape is specified to match the dimensions of our input data. Custom layers are then added for the 3-class classification. To prevent the weights of the pre-trained VGG16 base model from being updated during training, we freeze all the layers of the base model, after which the model is compiled, and trained using the training and validation datasets. ","metadata":{}},{"cell_type":"code","source":"base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax', kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.01))(x)  # 3 classes: 0 - not_drowsy, 5 - drowsy, 10 - neutral\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# The base is freezed\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# ! Actual training\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    validation_data=val_generator,\n    validation_steps=val_generator.samples // val_generator.batch_size,\n    epochs=5\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T17:00:20.282547Z","iopub.execute_input":"2024-12-09T17:00:20.282994Z","iopub.status.idle":"2024-12-09T18:06:06.697213Z","shell.execute_reply.started":"2024-12-09T17:00:20.282956Z","shell.execute_reply":"2024-12-09T18:06:06.695392Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_weights('drowsiness_weights.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-12-09T18:08:05.450469Z","iopub.execute_input":"2024-12-09T18:08:05.451604Z","iopub.status.idle":"2024-12-09T18:08:05.607794Z","shell.execute_reply.started":"2024-12-09T18:08:05.451548Z","shell.execute_reply":"2024-12-09T18:08:05.606540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Network Architecture ","metadata":{}},{"cell_type":"code","source":"#create bar chart to represent architecture model\nplot_model(model, to_file='model_drowsiness.png', show_shapes=True, show_layer_names=True)\nvisualkeras.layered_view(model, legend=True, draw_volume=False)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:08:46.470439Z","iopub.execute_input":"2024-12-09T18:08:46.471532Z","iopub.status.idle":"2024-12-09T18:08:47.842656Z","shell.execute_reply.started":"2024-12-09T18:08:46.471449Z","shell.execute_reply":"2024-12-09T18:08:47.841409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performance evaluation\nWith the accuracy surprisingly high, we've checked the appropriateness of labels (correct - the dataset is balanced; same amount of drowsy, neutral, and not drowsy images), and for potential data leakage issues (none - we've verified the integrity of train-test splits, all of the dataframe's feature column cells are unique).","metadata":{}},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test loss: {test_loss}')\nprint(f'Test accuracy: {test_accuracy}')\n\n# Accuracy and loss plots\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\n\n# Highest testing accuracy and its index\nmax_val_acc = max(val_accuracy)\nmin_val_acc = min(val_accuracy)\nmax_val_acc_index = val_accuracy.index(max_val_acc)\nmax_accuracy = max(accuracy)\nmin_loss = min(loss)\nmin_val_loss = min(val_loss)\nplt.plot(max_val_acc_index, max_val_acc, marker='o', color='lightpink')\n\n# Training and testing accuracy over epochs\nplt.plot(epochs, accuracy, \"g\", label=\"Training Accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"Testing Accuracy\")\nplt.legend()\nplt.title(\"Training and Testing Accuracy of the Model\\nHighest Testing Accuracy: {:.2f}%\".format(max_val_acc*100))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n# Plotting loss over epochs\nplt.plot(epochs, loss, \"g\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"Testing loss\")\nplt.legend()\nplt.title(\"Training and Testing Loss of the Model\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-09T18:08:59.440254Z","iopub.execute_input":"2024-12-09T18:08:59.440690Z","iopub.status.idle":"2024-12-09T18:14:36.486221Z","shell.execute_reply.started":"2024-12-09T18:08:59.440651Z","shell.execute_reply":"2024-12-09T18:14:36.484956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predicting on the test set\ntest_preds = []\ntest_labels = []\n\n# Iterating through the test data\nfor i in range(len(test_generator)):\n    images, labels = test_generator[i]\n    preds = model.predict(images)\n    test_preds.extend(np.argmax(preds, axis=1))\n    test_labels.extend(np.argmax(labels, axis=1))  # Get the true classes\n\n# Convert to numpy arrays for ease of use\ntest_preds = np.array(test_preds)\ntest_labels = np.array(test_labels)\n\n# Generate the confusion matrix\ncm = confusion_matrix(test_labels, test_preds)\nlabels = ['not_drowsy', 'neutral', 'drowsy']\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.title('Confusion Matrix for Drowsiness Detection')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T18:35:49.453202Z","iopub.execute_input":"2024-12-09T18:35:49.455610Z","iopub.status.idle":"2024-12-09T18:41:54.205071Z","shell.execute_reply.started":"2024-12-09T18:35:49.455541Z","shell.execute_reply":"2024-12-09T18:41:54.203906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Real-Time Driver Drowsiness Detection \nUsing the model defined above, we'll now implement a real-time DDS based on a webcam. The code captures live video frames using OpenCV, processes them to match the input requirements of the trained model, and performs predictions to determine drowsy vs alert state. The model's predictions are overlaid onto the video feed in real time, displaying the driver's status and confidence scores. \n\n*The system is interactive, allowing the user to view the annotated video feed and terminate the program by pressing the 'q' key.*","metadata":{}},{"cell_type":"code","source":"pip install h5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:45:51.712539Z","iopub.execute_input":"2024-12-10T08:45:51.712968Z","iopub.status.idle":"2024-12-10T08:46:02.143599Z","shell.execute_reply.started":"2024-12-10T08:45:51.712931Z","shell.execute_reply":"2024-12-10T08:46:02.142293Z"}},"outputs":[{"name":"stdout","text":"Collecting h5\n  Downloading h5-0.9.3-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: h5py>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from h5) (3.11.0)\nRequirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.10/site-packages (from h5) (1.26.4)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from h5) (4.12.2)\nDownloading h5-0.9.3-py3-none-any.whl (16 kB)\nInstalling collected packages: h5\nSuccessfully installed h5-0.9.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Rebuilding both models to omit steps above, with the weights already generated\n# -------------------------- Drowsiness Model --------------------------\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax', kernel_regularizer=regularizers.L1L2(l1=0.01, l2=0.01))(x)  # 3 classes: 0 - not_drowsy, 5 - drowsy, 10 - neutral\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n# The base is freezed\nfor layer in base_model.layers:\n    layer.trainable = False\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel_drowsiness_path = \"/kaggle/input/drowsiness-weights-weights-h5/drowsiness_weights.weights.h5\"\nmodel.load_weights(model_drowsiness_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:46:18.627941Z","iopub.execute_input":"2024-12-10T08:46:18.628368Z","iopub.status.idle":"2024-12-10T08:46:19.165323Z","shell.execute_reply.started":"2024-12-10T08:46:18.628330Z","shell.execute_reply":"2024-12-10T08:46:19.163792Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m model_drowsiness_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/drowsiness-weights-weights-h5/drowsiness_weights.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_drowsiness_path\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"],"ename":"OSError","evalue":"Unable to synchronously open file (file signature not found)","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Configuration for alert system\nconsecutive_drowsy_frames = 3  # Num of consecutive \"drowsy\" frames to trigger alert\ndrowsy_counter = 0\nmap_labels = {0: 'not_drowsy', 1: 'neutral', 2: 'drowsy'}\n\n# Defining the preprocessing function to match the model's input requirements. Returns a single adjusted frame. \ndef preprocess_frame(frame):\n  # Normalize \n  img = frame / 255\n  # Resize to match the size used during training\n  resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n  # Reshape the image to match the input shape \n  return resized.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n\n# Starting the camera; 0 - default camera\ncap = cv2.VideoCapture(0) \n\nprint(\"Press 'q' to quit the webcam stream.\")\nwhile cap.isOpened():\n  ret, frame = cap.read()  # Reading frame-by-frame\n  if not ret:\n    print(\"Failed to capture frame. Exiting...\")\n    break\n  \n  processed_frame = preprocess_frame(frame)\n\n  # Making a prediction\n  prediction_drowsy = model.predict(processed_frame)\n  drowsy = np.argmax(prediction_drowsy) \n  drowsy_label = map_labels[drowsy]\n  print(prediction_drowsy)\n  print(drowsy_label)\n\n  # Combine predictions. Final status will be drowsy if 30 cons frames are drowsy (/closed/yawn)\n  if drowsy_label == 'drowsy':  # must be drowsy and (closed eyes or yawning)\n    drowsy_counter += 1\n  else: \n    drowsy_counter = 0\n\n  # Checking if alert conditions are met\n  if drowsy_counter >= consecutive_drowsy_frames:\n    status = \"Drowsiness Detected\"\n    color = (0, 0, 255)  \n  else :\n    status = \"No Drowsiness Detected\"\n    color = (0, 255, 0)\n\n  # Annotating the frame\n  cv2.putText(frame, f\"Status: {status}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n  cv2.putText(frame, f\"Prediction confidence: {prediction_drowsy[0][drowsy]*100:.2f}\", (10, 70),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n  # Displaying the frame with annotations\n  cv2.imshow(\"Drowsy Driver Detection\", frame)\n\n  # Breaking the loop once 'q' is pressed\n  if cv2.waitKey(1) & 0xFF == ord('q'):\n    break\n\ncap.release()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:46:59.234353Z","iopub.execute_input":"2024-12-10T08:46:59.235107Z","iopub.status.idle":"2024-12-10T08:46:59.255083Z","shell.execute_reply.started":"2024-12-10T08:46:59.235067Z","shell.execute_reply":"2024-12-10T08:46:59.254077Z"}},"outputs":[{"name":"stdout","text":"Press 'q' to quit the webcam stream.\n","output_type":"stream"},{"name":"stderr","text":"[ WARN:0@358.604] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n[ERROR:0@358.606] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n","output_type":"stream"}],"execution_count":10}]}